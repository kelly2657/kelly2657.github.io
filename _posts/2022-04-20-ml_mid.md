---
title: '[Blog]핸즈온머신러닝 1~4 정리 및 요약'
layout: single
categories:
  - Study
tag:
  - Blog
  - machinelearning
toc: true
toc_label: "on this page"
toc_sticky: true
---
# 1장
> - **머신러닝**이란 ? 명시적인 프로그래밍 없이 컴퓨터가 스스로 데이터에 대해 학습하도록 하는 연구분야
> - 훈련셋 : 머신러닝 프로그램 훈련에 사용하는 데이터 셋
> - 샘플 : 훈련 데이터

> 머신러닝의 장점
> - 새로운 데이터로 시스템 쉽게 재훈련 시킬 수 있다.   
> - 전통적인 방식으로 해결 어려운 복잡한 문제를 해결할 수 있다.   
> - 데이터 마이닝 가능하다.

> 지도학습
> - 분류 : 값의 소속을 예측한다.
> - 회귀 : 값을 예측한다.

> __머신러닝 주요 도전과제__
> 1. 데이터의 크기가 클수록 모델 성능이 좋아진다.   
> 충분한 데이터로 모델을 훈련시켜서 과대적합이나 과소적합이 일어나지 않도록 한다.   
> 2. 모델이 잡음이나 편향에 민감해질 수 있으므로 적절한 데이터 정제가 필요하다.   

> __머신러닝 과정__
> 데이터 구하기 - 데이터 적재, 정제, 전처리 - 모델 선택과 훈련


# 2장
- 과제를 통해 알 수 있었던 점 : 데이터를 적절히 정제, 전처리하면 모델의 성능이 좋아진다 

> 다중회귀 : 여러 특성을 사용해서 예측한다.
> 
> 단변량회귀 : 한번에 한 종류의 값을 예측한다.   
> 다변량회귀 : 한번에 여러 종류의 값을 예측한다. 

> __모델성능 측정지표__
> RMSE(평균 제곱근 오차), MAE(평균 절대 오차) : 낮을 수록 좋음   
> RMSE가 너무 낮아 0이면 과대적합, 너무 높으면 과소적합일 수 있으므로 너무 높거나 낮아도 좋은게 아님

> 층화표집 : 계층의 적절한 샘플 추측   
> 상관관계
> - .corr()
> - -1~1로 나타냄
> - -1 : 매우 약한 선형 상관관계   
> - 0 : 매우 강한 음의 선형 상관관계 (상관관계 없음이 아니라 선형 상관관계가 나타나지 않는 것임)   
> - 1 : 매우 강한 양의 선형 상관관계

> __추정기__
> > fit() : 모델에 목적에 맞게 데이터셋을 계산한다.  
>  
> __변환기__   
> > fit()      
> > transform() : fit()메서드로 계산한 데이터셋을 변형   
> > fit_transform() : fit() -> transform()   
> > fit()과 fit_transform()은 훈련셋에 대해서만 사용함, transform()은 테스트셋, 검증셋, 새로운 데이터에 대해서 사용
>
> __예측기__
> > fit()   
> > predict() : fit()으로 계산한 값으로 타깃을 예측한다.   
> > score()predict()에서 예측한 타깃의 성능을 한다.   

> OneHotEncoding : 범주형특성을 수치형 특성으로 바꿔준다

> **스케일링**
> - 하지 않을 경우 모델이 작동하지 않음
> - **모든 데이터의 특성의 척도를 통일시켜준다.**
> - 정규화와 표준화
> - 경사하강법에서는 비용함수가 타원형이 된다.
> - **비용함수의 전역 최솟값으로 최단거리 수렴을 위해 사용됨.**

> **교차검증**
> - 훈련을 반복하는데 k번 반복하는 경우 데이터셋을 k개로 나누고 한번의 훈련마다 무작위로 1개의 데이터셋을 뽑아 테스트 셋으로 사용하고 나머지 k-1개의 데이터셋은 훈련셋으로 사용한다.
> - cross_val_score : 교차검증과정에서 훈련중인 모델의 성능을 측정한다.
> - scoring = "neg_mean_squared_error"키워드는 RMSE(평균제곱근오차)값을 음수로 출력하여 출력된 값이 높을수록 모델의 성능이 좋다고 판단.

> 선형회귀모델   
> 결정트리모델   
> 랜덤포레스트 모델(여러개의 결정트리모델을 동시에 훈련)   
> 앙상블기법: 성능이 좋은 여러개의 모델을 동시에 훈련시켜 그 평균값을 모델의 최종 성능으로 결정한다 - 랜덤포레스트모델

# 3장
- MNIST데이터셋
- 이진분류기 : A냐 아니냐(SGDClassfier-확률적경사하강법 분류기 사용)

> **분류기 성능 측정 기준**
> - **정확도**
>   - 특정 범주의 데이터의 양이 상대적으로 많은 경우에는 신뢰하기 어렵다.
> - **정밀도**
>   - 양성으로 예측된 것 중 실제로 양성인 비율
>   - tp/(tp+fp)
>   - 아이가 볼 수 있는 안전한 동영상으로 판단된 영상 중 실제로 안전한 영상인 것 
> - **재현율**
>   - 실제로 양성인 것 중 양성이라고 예측된 비율
>   - tp/(tp+fn)
>   - 실제 암 중 암이라고 진단된 경우
> - **ROC곡선의 AUC**
>     - 실제로 음성인 것들 중 양성이라고 예측된 비율
>     - ROC곡선 아래의 면적인 AUC가 1에 가까울수록 모델의 성능이 좋음

> **이진분류기->다중분류기**
> - 일대다 방식
>   - 한 데이터 당 예측될 수 있는 값의 성능을 모두 예측한 후 성능이 가장 좋게나온 값을 최종 예측으로 결정
> - 일대일 방식
>   - 일대일분류방식을 사용해 가장 많은 결투에서 이긴 값을 최종예측으로 결정

> 데이터 증식 : 가지고 있는 데이터를 조금씩 변형해서 많은 데이터로 만드는 것

# 4장
- 1인당 GDP와 OECD
  - 1인당 GDP : 입력 특성
  - OECD : 예측값
  - 기울기와 절편 : 파라미터 (가중치와 편향)

> __회귀모델의 목표는 비용함수를 최소화하는 파라미터(세타값)을 구하는 것__
> 1. 정규방정식, SVD활용
> 2. 경사하강법 사용
>   - 특성 수가 많거나 훈련셋의 크기가 커서 메모리에 한번에 담을 수 없는 경우 사용
>   - 일반적으로 선형회귀모델에 적용

> 경사하강법 용어
> - 허용오차 : 비용함수가 허용오차보다 작아지면 훈련 종료
> - 파라미터 : 훈련하면서 학습되는 파라미터
> - 비용함수의 그레디언트 벡터 : 방향과 크기를 가지고 있으며 이 벡터 방향의 반대 방향으로 움직이면 전역 최솟값에 빠르게 도달할 수 있음
> - 전역최솟값 : 비용함수의 전역 최솟값
> - 에포크 : 훈련셋의 데이터 대상으로 예측값 계산하는 모든과정. 허용오차와 반비례관계이다.
> - 하이퍼파라미터 : 모델선정시 정해지는 값
> - 비용함수 : 모델이 얼마나 좋지 않은지 평가하는 함수
> - 배차크기 : 그레디언트 벡터 계산 위해 사용되는 훈련데이터 수
> - 최적 학습 모델 : 비용함수를 최소화하고 효용함수를 최대화하는 파라미터를 가지며 최종적으로 훈련할 모델
> - 학습률 : 훈련과정에서 비용함수 파라미터 조정시 사용하는 조정비율이다. 너무 작으면 전역 최솟값 도달에 많은 시간이 걸리며 너무 크면 진동이 너무 커서 전역 최솟값에 수렴하기 어렵다

> __경사하강법__
> 1. 확률적 경사하강법(SGD)
> - 배치크기 = 1, 스텝크기 = 데이터크기
> - 대규모 데이터 처리 가능하며 외부 메모리 학습에 활용 가능
> - 파라미터 조정이 불안정할 수 있어 지역최솟값에 덜 민감하지만 같은 이유로 전역최솟값 수렴이 안될 수 있다.
> 2. 미니배치 경사하강법
> - 배치크기 = 2~수백
> - SGD보다 파라미터 움직임이 덜 규칙적이고 배치GD보다 빠른 학습
> - SGD보다 지역최솟값에 수렴할 위험 큼
> 3. 배치 경사하강법
> - 배치크기 = 데이터크기, 스텝크기 = 1
> > 배치-미니배치-확률적 
> > 순으로 최적의 파라미터에 수렴할 확률이 높고 훈련시간이 오래걸린다.

> **과소적합**
> - 편향이 높음 규제(알파)를 감소시켜야함
> - 훈련셋의 성능 : 훈련셋의 크기가 커질수록 RMSE도 증가하다가 훈련셋이 어느정도 커지면 RMSE는 거의 불변한다.     
> - 검증셋의 성능 : 훈련셋의 성능과 거의 일치한다.
>    
> **과대적합**
> - 분산이 커짐(데이터에 대한 민감도가 큼)
> - 훈련셋의 성능 : 훈련셋에 대한 RMSE가 낮다.
> - 검증셋의 성능 : 훈련셋의 성능과 차이가 난다.
> - 해결방법
>   - 훈련셋의 성능과 검증셋의 성능을 나타내는 두 그래프가 맞닿도록 훈련데이터를 추가한다.
>   - 모델에 규제를 가한다.

> 모델 일반화 오차
> - 편향(과소적합 유발), 분산(과대적합 유발), 축소불가능오차(잡음-데이터 정제 필요)

> **규제 사용 선형모델**
> _규제는 훈련과정에서만 사용한다._
> - 릿지회귀(l2)
>   - 특성 스케일링 필요
>   - 알파(규제)커질수록 데이터에 대한 민감도 떨어진다. 세타값이 작게 주어진다.
>   - 작은 세타 선호함
> - 라쏘회귀(l1)
>   - 중요하지 않은 특성을 빠르게 무시한다.
>   - MNIST와 같은 유용하지 않다고 판단되는 특성이 많은 데이터로 훈련할 때 사용한다
> - 엘라스틱 넷
>   - 라쏘회귀와 릿지회귀 합해 사용한다.
>   - 하이퍼파라미터(r)값을 가지고 어떤 회귀모델을 중점으로 사용할지 결정한다
>   - r값이 크면 라쏘회귀를 더 중점적으로 사용한다.
> > 어느정도 규제를 사용해야하며 일반적으로는 릿지회귀가 추천된다.   
> > 하지만 유용하지 않은 특성이 많다고 판단되는 경우 라쏘회귀나 엘라스틱넷을 추천하며   
> > 특성 수가 훈련데이터 수보다 크거나 몇 특성이 강하게 연관된 경우엔 엘라스틱넷을 추천한다.

> **조기종료**
> - 에포크를 많이 주면 모델이 데이터를 다 외우는 경우가 발생한다.   
> 에포크를 많이 주고 일정 간격마다 비용함수를 저장하면서   
> 비용함수가 감소하다가 증가하는 지점에서 모델을 조기종료 후 최적의 모델을 결정한다.
 
> __로지스틱 회귀__
> - 0~1 값을 예측해 0에 가까우면 음성, 1에 가까우면 양성으로 분류한다.
> - 선형회귀를 이용한다.
> - 이진분류만 지원
> - 시그모이드 함수를 사용한다. (0과 1에 가까워지지만 0과1이 되진 않음)
>   - t가0일 때 양성일 확률이 0.5이다.
> - 비용함수
>   - 양성인데 음성이라 분류시 비용함수에서 시그마(t)가 0.5보다 작을 때부터 비용 급격히 증가
>   - 음성인데 양성이라 분류시 비용함수에서 시그마(t)가 0.5보다 클 때부터 비용 급격히 증가

> __소프트맥스회귀__
> - 다중 로지스틱 회귀라고도 함
> - 로지스틱회귀를 일반화시켜 다중분류를 지원하도록 함
> - 샘플 들어오면 선형회귀 예측
> - 선형회귀 예측을 할 때 세타셋이 클래스 개수만큼 필요
> - 추정확률 = (특정 클래스에 들어갈 확률/모든 클래스에 들어갈 확률) 중 가장 추정확률 높은 클래스로 분류
