---
title: '[Blog]머신러닝 기말 정리'
layout: single
categories:
  - 시험 요약 정리
tag:
  - Blog
  - machinelearning
  - studyalone
toc: true
toc_label: "on this page"
toc_sticky: true
use_math: true
published : false
---
# 앙상블 학습
- 랜덤포레스트는 앙상블학습의 예
- 일반화성능이 좋은 모델을 만든다 = __편향과 분산이 최소화되는 모델__
  - 편향
    - 예측값과 정답이 떨어져있는 정도. 
    - 정답에 대한 잘못된 가정으로부터 유발, 편향 높으면 과소적합
  - 분산
    - 입력 샘플의 작은 변동에 반응하는 정도. 
    - 모델 복잡하면 편향 줄지만 분산 증가분산 크면 과대적합
  - 편향과 분산은 서로 `트레이드오프(trade-off)`되므로 적정한 모델을 찾아야함
- __베깅 기법__
  - 독립적으로 학습된 예측기 여러개의 예측값의 평균값을 최종예측값으로 함
  - __분산__ 줄임
  - 랜덤포레스트 모델
- __부스팅 기법__
  - 여러예측기 순차적으로 쌓아올려 예측값의 __편향__ 줄임

## 투표식 분류기
- 동일한 훈련셋에 대해 여러 분류모델을 이용한 앙상블학습 진행 후 직접/간접 투표로 예측값 결정
  - 직접투표 : 다수결 방식으로(각각의 예측기보다 좋은 성능의 모델 얻을 수 있음)
  - 간접투표 : 예측 확률값들의 평균값으로 예측값 결정
    - 모든 예측기가 `predict_proba()`메서드처럼 확률 예측하는 기능 지원해야함
- 두 방식에 큰 차이가 있진 않음
- `VotinhClassfier`, `VotingRegressor`
- 성능이 좋아지는 이유는 ❓
  - 다수결을 따라서 대체로 아주 나쁜 결과가 나올 수 없음 
(5.17)
## 베깅과 페이스팅
- 여러개의 동일 모델을 훈련셋의 다양한 부분집합을 대상으로 학습 시키는 방식  
- 베깅 : 중복 허용 샘플링(부분집합 선택)
- 페이스팅 : 중복 미허용 샘플링(부분집합 선택)
- 예측값
  - 분류 : 예측값의 최빈값
  - 회귀 : 예측값들의 평균값
- 병렬 훈련과 예측가능(다른 CPU나 서버 사용가능)
- 편향은 비슷하나 분산은 줄어듬(배깅이 표본 샘플링의 다양성을 많이 추가)
- __배깅__이 페이스팅보다 과대적합의 위험성이 줄어 __기본으로 사용함__
- 

> __사이킷런의 배깅과 페이스팅__
- `BaggingClassifier`, `BaggingRegressor`
- 배깅 방식(`bootstrap=True`). 페이스팅 방식을 사용하려면 `bootstrap=False` 로 지정
- 기본적으로 간접 투표 방식 사용.
```python
bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,
                            max_samples=100, random_state=42)
'''
500개 결정트리모델 앙상블 학습
100개의 훈련샘플 사용(배깅기법으로 중복 허용되어 실제로는 100개의 샘플을 사용하지 않을 수 있음)
n_jobs=-1 -> 사용할 CPU 수 지정, CPU모두 사용 의미
'''
```                     

> __🌟oob 평가__
- 배깅 기법에만 허용됨(페이스팅 기법에는 허용x)
- 중복을 허용하는 배깅기법에서 모델훈련에 선택이 되지 않는 훈련샘플이 생기고 이 훈련샘플을 `oob`라고 함.
- 이 `oob`를 검증셋으로 사용  
(이 검증셋과 훈련에 사용되지 않은 예측기들로 이루어진 앙상블 모델의 예측값으로 앙상블 모델의 성능을 검증하는데 사용)하는 것이 __oob평가__ 이다.

## 랜덤 패치와 랜덤 서브스페이스
- __랜덤 패치__ : 훈련샘플과 특성 모두를 대상으로 중복 허용. 임의의 샘플 & 특성 수만큼 샘플링해 학습
- __랜덤 서브스페이스__ : 전체 훈련세트를 학습 대상으로 함. 훈련 특성은 임의의 특성 수만큼 샘플링해 학습
> 모델을 다양하게 만들기 위해서 사용함
> 편향이 커지지만 분산 작아짐

## 랜덤 포레스트
- 배깅의 일부 방식(배깅 방식에서 모델을 결정트리모델로 결정하면 랜덤포레스트 모델)
- `RandomForestClassifier`, `RandomForestRegressor`
```python
rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,
                                 n_jobs=-1, random_state=42)
```                                 
```python
bag_clf = BaggingClassifier(
    DecisionTreeClassifier(max_features="sqrt", max_leaf_nodes=16),
    n_estimators=500, n_jobs=-1, random_state=42)
'''
max_features="sqrt"는 특성 수를 √n개만 무작위로 선택해 훈련
'''
```    
> __엑스트라 트리__
- (무작위성 강조된 이진 트리)
- 모델의 다양성을 위해 사용됨
- 특성과 특성값 모두 무작위로 선택해 최선의 임곗값을 찾는다.
- 랜덤포레스트보다 속도가 훨씬 빠르고(계산 적음), 편향이 높다. 하지만 분산은 상대적으로 낮다.
> __특성 중요도__
- 해당 특성을 사용한 마디가 평균적으로 불순도를 얼마나 감소시키는가 측정
  - 불순도 많이 줄어들수록 중요도 커짐

(5.24)
## 부스팅
> __에이다 부스트__
- 예측이 잘 되지 않는 샘플의 가중치를 높여 성능을 높임
- 하나의 모델을 훈련시키고 성능이 좋아지도록 모델을 수정해 다시 훈련(약점을 보완해가며 훈련)
- 같은 클래스의 모델의 조건을 바꿔주며 훈련한다.(각각 훈련하는 모델은 조건이 다르므로 다른 모델임)
- `fit()`메서드에 `sample_weight = `키워드인자가 존재. 샘플에 대한 가중치를 지정할 수 있는 것
  - 샘플 수 만큼의 리스트가 들어가고 리스트에 있는 수 자리에 해당하는 샘플을 그 수만큼 반복적으로 학습
  - __학습률__ : `sample_weight`가 너무 크게 주어지면 편향이 생길 수 있으므로 `sample_weight`값을 조절한다.
    - `경사하강법의 학습률`과는 다르지만모델의 변화에서 샘플 가중치의 적용 정도를 조절한다는 점에서 `경사하강법의 학습률`과 비슷함
- 사이킷런 모델 : `AdaBoostClassifier`, `AdaBoostRegressor`
- 코드
  - `n_estimators` : 부스팅 반복 횟수
  - `learning_rate` : 학습률(`sample_weight`변화 정도)
```python
ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=1), n_estimators=30,
    learning_rate=0.5, random_state=42)
```
> __그레디언트 부스팅__
- 원래 데이터와 예측값 사이의 오차(잔차)를 훈련셋으로 예측하며 결정트리모델을 만든다 
- 잔차에 대해 예측을 계속해 약점 보완해가며 모델을 만듬
  - 최종 예측값은 원래 예측값 + 잔차들에 대한 예측값 (계속 누적해감)
- `GradientBoostingRegressor`
  - 결정트리모델만 사용
- 학습률(`learning_rate`) : 잔차에 대한 예측 비율을 결정한다.(그 전 모델에 비해 얼마나 변화를 줄 것인지)
- __학습률과 축소규제__
  - (잔차를 줄여서 전달해줌)
  - 학습률이 작게 주어지면 모델이 많이 필요함(훈련을 더 많이 해야함 - 잔차가 안줄어들어서) 
    - 일반화 성능이 좋아짐(데이터에 민감하게 움직이지 않음)
  - 학습률을 작게 주고 모델을 많이 훈련하는게 일반화 성능에 좋음(데이터에 덜 민감해짐)
- __조기종료(`n_iter_no_change`)__
  - 학습률을 적게 주고 훈련 횟수를 크게 준다
```python
gbrt_best = GradientBoostingRegressor(
    max_depth=2, learning_rate=0.05, n_estimators=500,
    n_iter_no_change=10, random_state=42)
# 500번 학습하다가 연속적으로 10번 동안 성능 변화가 거의 없다면 조기종료
```
> _확률적 그레디언트 부스팅_
  - `subsample`하이퍼파라미터를 이용해 각 결정트리가 훈련에 사용할 훈련샘플의 비율지정(샘플이 많으면 시간 오래걸림)
  - 훈련셋 클 경우 사용
  - 훈련샘플은 매번 무작위로 선택
  - 훈련속도 높일 수 있고 편향은 높아지지만 모델 다양성이 커져 분산이 작아짐
    - 모델 경험이 많아지면 분산이 작아짐
> __히스토그램 그레디언트 부스팅__
- 결정트리모델을 사용하지만 구간을 나눠서 훈련하기 때문에 빠른 예측을 한다. (성능은 떨어지지만 그만큼의 장점 있음)
- `HistGradientBoostingClassifier`, `HistGradientBoostingRegressor`
- 최대 255개의 구간으로 나눠서 예측한다.
  - 구간이 나눠졌으므로 특성을 정렬할 필요가 없어져서 시간복잡도가 아주 많이 줄어듬)
 - 정확도는 떨어지지만 대용량 데이터 다룰 수 있다는 장점
- __장점__
  - 범주특성을 다룰 수 있음(`One-Hot Encoding`이 아니라 `Ordinal Encoding`를 사용해 기수를 사용해도 문제 없음 - 숫자에 의미를두지 않음)
  - 결측치 처리 가능(결측치는 255구간 외의 특별한 구간에 포함된다고 간주)
- 코드
```python
hgb_reg = make_pipeline(
    make_column_transformer((OrdinalEncoder(), ["ocean_proximity"]), remainder="passthrough"),
                      HistGradientBoostingRegressor(categorical_features=[0], random_state=42))

hgb_reg.fit(housing, housing_labels)

# "ocean_proximity"만 OrdinarlEnCoding
# categorical_features=[0] : 범주형특성의 위치를 알려줌(순서가 중요하지 않다는 것을 알려줌)
```
# 차원축소
## 차원의 저주
- 샘플의 특성이 너무 많아 학습에 어려움이 생김(두 데이터 사이의 거리가 멀어짐)
  = 일반화성능이 떨어짐

## 차원축소기법
1. 사영기법
- n차원 데이터셋을 d(<n)차원 데이터셋으로 사영하는 기법
- 어느 각도를 사용할지를 결정해야함
- 항상 간단한 데이터셋을 만들진 않음(롤케이크 데이터셋 등)
2. 다양체 학습 기법
- 고차원 공간에서 공간을 변형시켜 생성할 수 있는 공간이 다양체(롤케이크 데이터셋을 펴서 간단한 데이터셋을 만듬)

## 주성분 분석 PCA(다시 들어보기)
- 주성분이 무엇인지 찾는것
- 주성분 : 사영시켰을 떄 원 데이터의 분산정도를 유지시켜주는 축
  - 첫째 주성분 : 분산을 최대한 보존하는 축
  - 둘째 주성분 : 첫째 주성분과 수직을 이루며 아직 설명되지 않은 분산을 최대한 보존하는 축
  - 셋째 주성분 : n-1주성분과 수직이루며 아직 설명되지 않은 분산을 최대한 보존하는 축
- PCA 모델은 `SVD`방식을 사용함

> 설명분산비율
- 원 데이터 분산에 대한 유지비율(원데이터분산과 얼마나 유사한가)
- 적절한 차원 : 원 데이터 차원보다 낮은 차원까지 축을 더해서 원데이터 분산을유지할 수 있음
- mnist데이터셋에 대해서는 784축 중 154개의 축만 사용해도 95%의 원데이터 분산을 유지할 수 있음
- `PCA(n_components=부동소수점(기본값 0.95))`부동소수점 비율만큼 지정 가능함(정수 지정으로 축의 개수 지정 가능)
- 이미지 축소 수 다시 확대해도 큰영향없음(차원축소가 이미지에도 활용됨)

> 랜덤 PCA (임의사영과 다름)
- 성능이 조금 떨어지지만 빠르게 주성분 찾아줌
- 특성수 진짜 많을 떄 사용

> 점진적 PCA
- 데이터 수 진짜 많을 때 사용
- `IncrementalPCA(n_component=154)`
- 훈련에` partial_fit()`
- `memmap`클래스 : 파일로 저장된 매우 큰 데이터셋을 메모리에 들어있는것처럼 취급

## 임의 사영
- 주성분 분석에서 시간이 오래걸리므로 고차원 데이터를 적절한 크기의 저차원으로 임의 사영을 
- `GaussianRnadomProjection`, `SparseRandomProjection`(희소행렬사용, 더 큰 데이터와 빠른 속도)
## LLE
- 국소적 선형 임베딩
- 국소적으로는 데이터가 선형관계라는 가설 이용
- 국소적 관계가 가장 잘 보존되는 훈련셋의 저차원표현 찾음
