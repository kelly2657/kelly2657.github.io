---
title: '[Blog]머신러닝 기말 정리_1'
layout: single
categories:
  - 시험 요약 정리
tag:
  - Blog
  - machinelearning
  - studyalone
toc: true
toc_label: "on this page"
toc_sticky: true
use_math: true
published : false
---

# 서포트벡터머신
(0502)

- 서포트 벡터 머신은 일반화를 위해 사용됨
- 마진 : 결정 경계 도로의 폭 
- 비선형 분류가 오래걸림
- 서포트 벡터 : 결정경계 도로의 가장자리에 위치한 점들
![사진](/assets/img/svm_1.jpg)

- 서포트벡터머신에서도 스케일링이 중요
- 큰 마진 분류(large margin classfication) : 마진을 최대로 하는 분류

## 🌟선형 SVM 분류
1. __하드마진분류__ : 선형분류가 가능한 경우 가능. 결정경계 도로(마진)에 아무 데이터가 없고 선형 분류로 100% 완벽하게 분류 가능
2. __소프트마진분류__ : 선형 분류로 100% 완벽하게 분류 불가능. 어느정도의 마진 오류를 허용하며 오류를 최대한 줄이며 도로 폭(마진)을 최대로 하는 분류.
  - 마진오류 : 결정경계 도로 상에 데이터가 있거나 결정경계를 넘어 데이터가 섞인 것


- __LinearSVC클래스__ : 선형회귀 지원하는 소프트벡터머신 분류기
  - __C__(규제의 강도) : 항상 양수여야하며 커질수록 규제 약해짐.클수록 결정경계의 폭이 좁아짐
- __SVC클래스__(배치경사하강법 사용)
  - 하이퍼파라미터 kernel "linear"지정해 선형분류 가능. 비선형 분류도 가능함(but 시간 오래걸림).
- __SGDClassifier 클래스__(확률적 경사하강법)
  - 선형분류, 로지스틱 회귀 가능. 하이퍼파라미터 `loss="hinge"` 지정해 서포트벡터머신 사용 가능
  - `hinge함수`는 손실함수를 나타낸다. (계산량 적음)

![사진](/assets/img/svm_2.png)

## 비선형 SVM 분류
  예시 ) `moons`데이터셋
1. __특성 추가 + 선형 SVC__
- 특성이나 데이터가 많은 경우 분류에 시간이 오래걸림, 비용 증가 등의 실현 문제점 발생 
- 다항 특성 추가 후 원하는 모델 선택
2. __🌟SVC + 커널 트릭__
- `kernel = "poly"` 하이퍼파라미터 추가로 실제로 항을 추가하지 않지만 추가한 것과 같은 효과 냄.
- `coef0` : 클수록 고차항의 중요도가 높아짐
- `C` : 규제의 정도 
3. __🌟유사도 특성__
- 임의의 샘플을 랜드마크(m)로 지정 후 특정 샘플과 각 샘플이 얼마나 유사한지를 나타내는 값이다.
- 어떤 샘플을 랜드마크로 지정할 것인가 ? ---> _모든 훈련샘플을 랜드마크로 지정해본다 = RBF_
  


(0503)  
4. __🌟가우시간 RBF 커널__
- __랜드마크에 대한 유사도를 측정한 후 특성으로 추가__ _(= 훈련데이터 수만큼 새로운 특성 추가)_
- 즉, __새로운 특성은 랜드마크와의 유사도__ _(다른 훈련셋과 얼마나 유사하냐. 얼마나 가깝냐.)_ 측정 후 
- _어떤 것을 랜드마크로 지정 > 모든 훈련셋을 랜드마크로 사용해봄. (훈련셋의 개수가 10000개면 특성 10000개가 추가됨.)_
- (참고)가우시안 방사기저함수(RBF)
  - ϕ(x,m)=exp(−γ∥x−m∥<sup>2</sup>)로 나타냄 
  - γ클수록 가까운 샘플에만 영향을 미침

> `γ`가 크면 가까운 샘플들만 유사하다고 분류  
> 규제 `C`가 크면 데이터에 민감해져 모델이 복잡해짐

    (참고)
    좋은 모델을 찾고 그리드탐색, 랜덤탐색으로 모델 튜딩이나 앙상블 기법으로 더 좋은 성능을 찾아냄
# SVM 회귀
- 결정경계 도로 위에 가능한 많은 데이터가 위치하도록 결정경계를 만듬 _(분류와의 차이)_
- ϵ 클수록 도록폭 넓어짐(도로 위에 있는 데이터가 많아짐 - 많다고 좋은 모델은 아님)

(0509)
# SVM 이론
- 결정합숫값 : h(x)= w<sup>t</sup>+b=w<sub>1</sub>x<sub>1</sub>+⋅⋅⋅+w<sub>n</sub>x<sub>n</sub>+b
> w<sub>i</sub>가 평면의 기울기를 결정함.  
> 붓꽃 데이터에선 |w<sub>1</sub>|+|w<sub>2</sub>|가 평면의 기울기를 결정하며 클수록 결정경계도로가 좁아짐 
- 결정함숫값이 음수이면 음성 (`ŷ = 0`), 0이나 양수이면 양성(`ŷ = 0`)으로 분류한다
- 좋은 `가중치(w)`와 `편향(b)`을 찾는 것이 필요하다
- __결정경계에 따라 도로의 넓이가 달라지기 때문에 결정경계를 잘 정하는 것이 중요(도로의 넓이가 넓을수록 일반화 성능증가)__
- _두 평면이 만나는 각도에 따라 결정경계가 달라짐_

> __SVM이 해결해야할 문제__ 
- (조건1) __결정경계 중심으로 분류 가능해야함__
- (조건2) __결정 경계면의 기울기(절댓값 |w<sub>1</sub>|+|w<sub>2</sub>|)를 최소로 만들어야함__(==||w||최소로 만들기, ||w||는 루트(w<sub>1</sub><sup>2</sup>+w<sub>2</sub><sup>2</sup>), (w<sub>1</sub>, w<sub>2</sub>)로 이루어진 벡터)

  나머지 내용은 강의 5-5참고하기

- 규제 `C`가 세지면 도로폭 넓어짐
- 손실함수를 완전히 만족시키면 목표로하는 조건식을 만족하기 어려워짐(모순적)
- 소프트 마진 선형SVM분류기에서 `ξ^(i)`값이 설명하는 부분은 허용오차를 말한다.
- 손실함수와 목표로하는 조건을 만족하는 w와 ξ값을 찾으면 적절한 결정경계도로를 만들 수 있음.

---

__쌍대문제__
- 의미가 같지만 다른 문제를 쌍대문제(원문제와 의미가 같은 다른 문제를 쌍대문제라고 함)
  - LinearSVC, LinearSVR의 dual하이퍼파라미터를 사용해 true로 지정하면 쌍대문제를, false로 지정하면 원문제를 푼다. 훈련샘플의 수가 특성 수보다 큰 경우는 false로 지정한다.(ture가 기본값)

# 결정트리
## 분류
- 루트(부모x 노드), 리프(부모 있는 노드)
- 지니불순도나 엔트로피를 최소화하는 기준값으로 리프노드 분류함
- 노드에 포함 가능한 내용
  - 분류 기준(자식노드 분류기준, 0~2개로 나눠질 수 있음. 부모 노드에만 포함됨)
  - gini(노드의 지니불순도 측정값, 낮으면 좋음)
  - samples(해당 노드에 포함 샘플 수)
  - value(해당 노드에 포함된샘플들의 실제 클래스별 개수, 타깃 정보 활용)
  - class(각 클래스별 비율 계해 가장 높은 비율의 클래스, 동일 비율이면 낮은 인덱스 선정)
- 화이트박스 모델임  
(5.10)
- 노드의 지니불순도가 0이 아니면 노드를 더 분류할 수 있음
- __n번째 노드 지니불순도__ 계산 : 1 ➖ (n번째 value의 1번째 클래스 샘플 수 / 현재 노드의 샘플 수)<sup>2</sup> ➖ (n번째 value의 2번째 클래스 샘플 수 / 현재 노드의 샘플 수)<sup>2</sup> ➖ ⋅⋅⋅⋅⋅⋅  
> 결정경계
- max_depth에 제한을 두지 않으면 모두 분류가 가능하지만 이는 좋은 것이 아니다. 과대적합의 가능성이 높아짐.

> __클래스 확률 추정__
> - 노드의 value값으로 샘플들의 클래스별 비율을 추정함

## CART훈련 알고리즘
- 어떤 기준으로 분류 기준을 정하느냐 ? 
  - 무작위나 모든 가능성(지니불순도)을 계산해 가장 좋은(가장 낮은) 기준을 정함
  - 꽃잎의 너비 등 특성 선택은 무작위로 하나 그 특성의 값은 모든 값을 계산해 가장 좋은 기준을 정한다.
- 어떤 기준으로 분류를 할지 정하는 알고리즘(이를 통해 이진트리 만들어냄)
- 손실함수(최소로 만들어야함) 
  - = (오른쪽 자식노드의 지니불순도 × 오른쪽 자식노드의 샘플 수) ➕ (왼쪽 자식노드의 지니불순도 × 왼쪽 자식노드의 샘플 수) 
  - __지니불순도 최소화하는 최적의 기준값을 찾아내야함__ ➡️ 현재의 기준값이 현재로써는 최적이나 최종적으로 최적이라는 보장은 없음 __(탐욕 알고리즘 사용)__

> __DecisionTreeClassfier의 `criterion = 'entropy'`__  
> - 지니불순도와 엔트로피 중 선택 가능함  
> - 엔트로피가 시간이 더 걸림(보통 지니불순도를 사용함)
> - 엔트로피가 트리의 균형을 더 잘 맞출 수 있음  
> - entropy : 변화가 클 수록 높음(샘플의 무질서 정도를 측정)  

## 규제 파라미터
- max_depth 등
- 없으면 지니불순도가 0일 때까지 분류.(이는 과대적합모델로 좋지 않음)
- `min_~`를 증가시키거나 `max_~`를 감소시킬 때 규제 강해짐(과대적합 막음)

(5.16)
## 회귀
- x축 기준으로 쪼갬 ➡️ 해당 구간의 y의 평균값을 예측값으로 결정
- 노드에 포함 가능한 내용
  - 분류 기준(자식노드 분류기준, 0~2개로 나눠질 수 있음. 부모 노드에만 포함됨)
  - squared_error : 해당 노드에 대한 mse(데이터 포함 기준이 해당노드 부모의 분류기준으로 결정됨)
  - value : 해당 노드에 포함된 데이터의 평균값(루트노트 value는 전체 데이터 평균값)
    - 노드에 포함된 데이터는 부모노드의 기준에 의하는 것 주의
  - samples : (해당 노드에 포함 샘플 수)


> __회귀용 CART알고리즘의 비용함수__
- __CART알고리즘__ : 탐욕 기법으로 비용함수(`MSE`)를 최소로 만드는 기준 결정
- mse = (노드의 `value`값 - 각 데이터 값)<sup>2</sup>의 모든 합/`samples`

> __회귀 결정트리 규제__
- 규제 없으면 과대적합 발생
- 수평선이 만들어지지 않음
- 규제를 주지 않으면 100% 예측
- 시간을 충분히 줄 경우 데이터에 대한 예측을 완벽하게함(좋지 않음 - 새로운 데이터에 대한 완벽한 예측은 어려움)


> ⭐⭐ __일반화 성능__  
> 새로운 데이터에 대해 잘 예측하는 성능.  
> 이미 훈련했던 데이터 중 새로운 데이터와 가장 가까운 값을 찾아 예측값을 제공했을 때 이 값에 대해 융통성이 있다면 일반화 성능이 높음

## 결정트리 단점
- _규제 적당히 주면 과대적합 피할 수 있음_
- __훈련셋에 대한 회전민감도__
  - 계단식으로 분류하므로 시간이 오래걸리고 훈련셋에 회전율을 조금만 가해도 결정경계가 많이 달라짐. 
  - `PCA()` : 회전율에 대한 전처리
    - 회전율이 가해진 데이터셋에 다시 반대로 회전율을 가해 특성 사이의 연관성을 약화시키고 결정트리 학습에 도움을 준다.
- 분산이 높음.
  - 훈련셋이나 하이퍼파라미터가 조금만 달라져도 완전히 다른 결정트리가 훈련될 수 있다.

# 앙상블 학습
- 랜덤포레스트는 앙상블학습의 예
- 일반화성능이 좋은 모델을 만든다 = __편향과 분산이 최소화되는 모델__
  - 편향
    - 예측값과 정답이 떨어져있는 정도. 
    - 정답에 대한 잘못된 가정으로부터 유발, 편향 높으면 과소적합
  - 분산
    - 입력 샘플의 작은 변동에 반응하는 정도. 
    - 모델 복잡하면 편향 줄지만 분산 증가분산 크면 과대적합
  - 편향과 분산은 서로 `트레이드오프(trade-off)`되므로 적정한 모델을 찾아야함
- __베깅 기법__
  - 독립적으로 학습된 예측기 여러개의 예측값의 평균값을 최종예측값으로 함
  - __분산__ 줄임
  - 랜덤포레스트 모델
- __부스팅 기법__
  - 여러예측기 순차적으로 쌓아올려 예측값의 __편향__ 줄임

## 투표식 분류기
- 동일한 훈련셋에 대해 여러 분류모델을 이용한 앙상블학습 진행 후 직접/간접 투표로 예측값 결정
  - 직접투표 : 다수결 방식으로(각각의 예측기보다 좋은 성능의 모델 얻을 수 있음)
  - 간접투표 : 예측 확률값들의 평균값으로 예측값 결정
    - 모든 예측기가 `predict_proba()`메서드처럼 확률 예측하는 기능 지원해야함
- 두 방식에 큰 차이가 있진 않음
- `VotinhClassfier`, `VotingRegressor`
- 성능이 좋아지는 이유는 ❓
  - 다수결을 따라서 대체로 아주 나쁜 결과가 나올 수 없음 
(5.17)
## 베깅과 페이스팅
- 여러개의 동일 모델을 훈련셋의 다양한 부분집합을 대상으로 학습 시키는 방식  
- 베깅 : 중복 허용 샘플링(부분집합 선택)
- 페이스팅 : 중복 미허용 샘플링(부분집합 선택)
- 예측값
  - 분류 : 예측값의 최빈값
  - 회귀 : 예측값들의 평균값
- 병렬 훈련과 예측가능(다른 CPU나 서버 사용가능)
- 편향은 비슷하나 분산은 줄어듬(배깅이 표본 샘플링의 다양성을 많이 추가)
- __배깅__이 페이스팅보다 과대적합의 위험성이 줄어 __기본으로 사용함__
- 

> __사이킷런의 배깅과 페이스팅__
- `BaggingClassifier`, `BaggingRegressor`
- 배깅 방식(`bootstrap=True`). 페이스팅 방식을 사용하려면 `bootstrap=False` 로 지정
- 기본적으로 간접 투표 방식 사용.
```python
bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,
                            max_samples=100, random_state=42)
'''
500개 결정트리모델 앙상블 학습
100개의 훈련샘플 사용(배깅기법으로 중복 허용되어 실제로는 100개의 샘플을 사용하지 않을 수 있음)
n_jobs=-1 -> 사용할 CPU 수 지정, CPU모두 사용 의미
'''
```                     

> __🌟oob 평가__
- 배깅 기법에만 허용됨(페이스팅 기법에는 허용x)
- 중복을 허용하는 배깅기법에서 모델훈련에 선택이 되지 않는 훈련샘플이 생기고 이 훈련샘플을 `oob`라고 함.
- 이 `oob`를 검증셋으로 사용  
(이 검증셋과 훈련에 사용되지 않은 예측기들로 이루어진 앙상블 모델의 예측값으로 앙상블 모델의 성능을 검증하는데 사용)하는 것이 __oob평가__ 이다.

## 랜덤 패치와 랜덤 서브스페이스
- __랜덤 패치__ : 훈련샘플과 특성 모두를 대상으로 중복 허용. 임의의 샘플 & 특성 수만큼 샘플링해 학습
- __랜덤 서브스페이스__ : 전체 훈련세트를 학습 대상으로 함. 훈련 특성은 임의의 특성 수만큼 샘플링해 학습
> 모델을 다양하게 만들기 위해서 사용함
> 편향이 커지지만 분산 작아짐

## 랜덤 포레스트
- 배깅의 일부 방식(배깅 방식에서 모델을 결정트리모델로 결정하면 랜덤포레스트 모델)
- `RandomForestClassifier`, `RandomForestRegressor`
```python
rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,
                                 n_jobs=-1, random_state=42)
```                                 
```python
bag_clf = BaggingClassifier(
    DecisionTreeClassifier(max_features="sqrt", max_leaf_nodes=16),
    n_estimators=500, n_jobs=-1, random_state=42)
'''
max_features="sqrt"는 특성 수를 √n개만 무작위로 선택해 훈련
'''
```    
> __엑스트라 트리__
- (무작위성 강조된 이진 트리)
- 모델의 다양성을 위해 사용됨
- 특성과 특성값 모두 무작위로 선택해 최선의 임곗값을 찾는다.
- 랜덤포레스트보다 속도가 훨씬 빠르고(계산 적음), 편향이 높다. 하지만 분산은 상대적으로 낮다.
> __특성 중요도__
- 해당 특성을 사용한 마디가 평균적으로 불순도를 얼마나 감소시키는가 측정
  - 불순도 많이 줄어들수록 중요도 커짐

(5.24)
## 부스팅
> __에이다 부스트__
- 예측이 잘 되지 않는 샘플의 가중치를 높여 성능을 높임
- 하나의 모델을 훈련시키고 성능이 좋아지도록 모델을 수정해 다시 훈련(약점을 보완해가며 훈련)
- 같은 클래스의 모델의 조건을 바꿔주며 훈련한다.(각각 훈련하는 모델은 조건이 다르므로 다른 모델임)
- `fit()`메서드에 `sample_weight = `키워드인자가 존재. 샘플에 대한 가중치를 지정할 수 있는 것
  - 샘플 수 만큼의 리스트가 들어가고 리스트에 있는 수 자리에 해당하는 샘플을 그 수만큼 반복적으로 학습
  - __학습률__ : `sample_weight`가 너무 크게 주어지면 편향이 생길 수 있으므로 `sample_weight`값을 조절한다.
    - `경사하강법의 학습률`과는 다르지만모델의 변화에서 샘플 가중치의 적용 정도를 조절한다는 점에서 `경사하강법의 학습률`과 비슷함
- 사이킷런 모델 : `AdaBoostClassifier`, `AdaBoostRegressor`
- 코드
  - `n_estimators` : 부스팅 반복 횟수
  - `learning_rate` : 학습률(`sample_weight`변화 정도)
```python
ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=1), n_estimators=30,
    learning_rate=0.5, random_state=42)
```
> __그레디언트 부스팅__
- 원래 데이터와 예측값 사이의 오차(잔차)를 훈련셋으로 예측하며 결정트리모델을 만든다 
- 잔차에 대해 예측을 계속해 약점 보완해가며 모델을 만듬
  - 최종 예측값은 원래 예측값 + 잔차들에 대한 예측값 (계속 누적해감)
- `GradientBoostingRegressor`
  - 결정트리모델만 사용
- 학습률(`learning_rate`) : 잔차에 대한 예측 비율을 결정한다.(그 전 모델에 비해 얼마나 변화를 줄 것인지)
- __학습률과 축소규제__
  - (잔차를 줄여서 전달해줌)
  - 학습률이 작게 주어지면 모델이 많이 필요함(훈련을 더 많이 해야함 - 잔차가 안줄어들어서) 
    - 일반화 성능이 좋아짐(데이터에 민감하게 움직이지 않음)
  - 학습률을 작게 주고 모델을 많이 훈련하는게 일반화 성능에 좋음(데이터에 덜 민감해짐)
- __조기종료(`n_iter_no_change`)__
  - 학습률을 적게 주고 훈련 횟수를 크게 준다
```python
gbrt_best = GradientBoostingRegressor(
    max_depth=2, learning_rate=0.05, n_estimators=500,
    n_iter_no_change=10, random_state=42)
# 500번 학습하다가 연속적으로 10번 동안 성능 변화가 거의 없다면 조기종료
```
> _확률적 그레디언트 부스팅_
  - `subsample`하이퍼파라미터를 이용해 각 결정트리가 훈련에 사용할 훈련샘플의 비율지정(샘플이 많으면 시간 오래걸림)
  - 훈련셋 클 경우 사용
  - 훈련샘플은 매번 무작위로 선택
  - 훈련속도 높일 수 있고 편향은 높아지지만 모델 다양성이 커져 분산이 작아짐
    - 모델 경험이 많아지면 분산이 작아짐
> __히스토그램 그레디언트 부스팅__
- 결정트리모델을 사용하지만 구간을 나눠서 훈련하기 때문에 빠른 예측을 한다. (성능은 떨어지지만 그만큼의 장점 있음)
- `HistGradientBoostingClassifier`, `HistGradientBoostingRegressor`
- 최대 255개의 구간으로 나눠서 예측한다.
  - 구간이 나눠졌으므로 특성을 정렬할 필요가 없어져서 시간복잡도가 아주 많이 줄어듬)
 - 정확도는 떨어지지만 대용량 데이터 다룰 수 있다는 장점
- __장점__
  - 범주특성을 다룰 수 있음(`One-Hot Encoding`이 아니라 `Ordinal Encoding`를 사용해 기수를 사용해도 문제 없음 - 숫자에 의미를두지 않음)
  - 결측치 처리 가능(결측치는 255구간 외의 특별한 구간에 포함된다고 간주)
- 코드
```python
hgb_reg = make_pipeline(
    make_column_transformer((OrdinalEncoder(), ["ocean_proximity"]), remainder="passthrough"),
                      HistGradientBoostingRegressor(categorical_features=[0], random_state=42))

hgb_reg.fit(housing, housing_labels)

# "ocean_proximity"만 OrdinarlEnCoding
# categorical_features=[0] : 범주형특성의 위치를 알려줌(순서가 중요하지 않다는 것을 알려줌)
```
# 차원축소
## 차원의 저주
- 샘플의 특성이 너무 많아 학습에 어려움이 생김(두 데이터 사이의 거리가 멀어짐)
  = 일반화성능이 떨어짐

## 차원축소기법
1. 사영기법
- n차원 데이터셋을 d(<n)차원 데이터셋으로 사영하는 기법
- 어느 각도를 사용할지를 결정해야함
- 항상 간단한 데이터셋을 만들진 않음(롤케이크 데이터셋 등)
2. 다양체 학습 기법
- 고차원 공간에서 공간을 변형시켜 생성할 수 있는 공간이 다양체(롤케이크 데이터셋을 펴서 간단한 데이터셋을 만듬)

## 주성분 분석 PCA(다시 들어보기)
- 주성분이 무엇인지 찾는것
- 주성분 : 사영시켰을 떄 원 데이터의 분산정도를 유지시켜주는 축
- 첫째 주성분 : 분산을 최대한 보존하는 축
- 둘째 주성분 : 첫째 주성분과 수직을 이루며 아직 설명되지 않은 분산을 최대한 보존하는 축
-셋째 주성분 : n-1주성분과 수직이루며 아직 설명되지 않은 분산을 최대한 보존하는 축
> 설명분산비율
- 원 데이터 분산에 대한 유지비율(원데이터분산과 얼마나 유사한가)
- 적절한 차원 : 원 데이터 차원보다 낮은 차원까지 축을 더해서 원데이터 분산을유지할 수 있음
- mnist데이터셋에 대해서는 784축 중 154개의 축만 사용해도 95%의 원데이터 분산을 유지할 수 있음
- `PCA(n_components=부동소수점(기본값 0.95))`부동소수점 비율만큼 지정 가능함(정수 지정으로 축의 개수 지정 가능)
- 이미지 축소 수 다시 확대해도 큰영향없음(차원축소가 이미지에도 활용됨)
> 랜덤 PCA (임의사영과 다름)
- 성능이 조금 떨어지지만 빠르게 주성분 찾아줌
- 특성수 진짜 많을 떄 사용
> 점진적 PCA
- 데이터 수 진짜 많을 때 사용
- `IncrementalPCA(n_component=154)`
- 훈련에` partial_fit()`
- `memmap`클래스 : 파일로 저장된 매우 큰 데이터셋을 메모리에 들어있는것처럼 취급

## 임의 사영
- 주성분 분석에서 시간이 오래걸리므로 고차원 데이터를 적절한 크기의 저차원으로 임의 사영을 
- `GaussianRnadomProjection`, `SparseRandomProjection`(희소행렬사용, 더 큰 데이터와 빠른 속도)
## LLE
- 국소적 선형 임베딩
- 국소적으로는 데이터가 선형관계라는 가설 이용
- 국소적 관계가 가장 잘 보존되는 훈련셋의 저차원표현 찾음
