---
title: '[Blog]머신러닝 기말 정리'
layout: single
categories:
  - 시험 요약 정리
tag:
  - Blog
  - machinelearning
  - studyalone
toc: true
toc_label: "on this page"
toc_sticky: true
use_math: true
published : false
---

# 서포트벡터머신
(0502)

- 서포트 벡터 머신은 일반화를 위해 사용됨
- 마진 : 결정 경계 도로의 폭 
- 비선형 분류가 오래걸림
- 서포트 벡터 : 결정경계 도로의 가장자리에 위치한 점들
![사진](/assets/img/svm_1.jpg)

- 서포트벡터머신에서도 스케일링이 중요
- 큰 마진 분류(large margin classfication) : 마진을 최대로 하는 분류

## 🌟선형 SVM 분류
1. __하드마진분류__ : 선형분류가 가능한 경우 가능. 결정경계 도로(마진)에 아무 데이터가 없고 선형 분류로 100% 완벽하게 분류 가능
2. __소프트마진분류__ : 선형 분류로 100% 완벽하게 분류 불가능. 어느정도의 마진 오류를 허용하며 오류를 최대한 줄이며 도로 폭(마진)을 최대로 하는 분류.
  - 마진오류 : 결정경계 도로 상에 데이터가 있거나 결정경계를 넘어 데이터가 섞인 것


- __LinearSVC클래스__ : 선형회귀 지원하는 소프트벡터머신 분류기
  - __C__(규제의 강도) : 항상 양수여야하며 커질수록 규제 약해짐.클수록 결정경계의 폭이 좁아짐
- __SVC클래스__(배치경사하강법 사용)
  - 하이퍼파라미터 kernel "linear"지정해 선형분류 가능. 비선형 분류도 가능함(but 시간 오래걸림).
- __SGDClassifier 클래스__(확률적 경사하강법)
  - 선형분류, 로지스틱 회귀 가능. 하이퍼파라미터 `loss="hinge"` 지정해 서포트벡터머신 사용 가능
  - `hinge함수`는 손실함수를 나타낸다. (계산량 적음)
![사진](/assets/img/svm_2.png)

## 비선형 SVM 분류
  예시 ) `moons`데이터셋
1. __특성 추가 + 선형 SVC__
- 특성이나 데이터가 많은 경우 분류에 시간이 오래걸림, 비용 증가 등의 실현 문제점 발생 
- 다항 특성 추가 후 원하는 모델 선택
2. __🌟SVC + 커널 트릭__
- `kernel = "poly"` 하이퍼파라미터 추가로 실제로 항을 추가하지 않지만 추가한 것과 같은 효과 냄.
- `coef0` : 클수록 고차항의 중요도가 높아짐
- `C` : 규제의 정도 
3. __🌟유사도 특성__
- 임의의 샘플을 랜드마크(m)로 지정 후 특정 샘플과 각 샘플이 얼마나 유사한지를 나타내는 값이다.
- 어떤 샘플을 랜드마크로 지정할 것인가 ? ---> _모든 훈련샘플을 랜드마크로 지정해본다 = RBF_
  


(0503)  
4. __🌟가우시간 RBF 커널__
- __랜드마크에 대한 유사도를 측정한 후 특성으로 추가__ _(= 훈련데이터 수만큼 새로운 특성 추가)_
- 즉, __새로운 특성은 랜드마크와의 유사도__ _(다른 훈련셋과 얼마나 유사하냐. 얼마나 가깝냐.)_ 측정 후 
- _어떤 것을 랜드마크로 지정 > 모든 훈련셋을 랜드마크로 사용해봄. (훈련셋의 개수가 10000개면 특성 10000개가 추가됨.)_
- (참고)가우시안 방사기저함수(RBF)
  - ϕ(x,m)=exp(−γ∥x−m∥<sup>2</sup>)로 나타냄 
  - γ클수록 가까운 샘플에만 영향을 미침

> `γ`가 크면 가까운 샘플들만 유사하다고 분류  
> 규제 `C`가 크면 데이터에 민감해져 모델이 복잡해짐

    (참고)
    좋은 모델을 찾고 그리드탐색, 랜덤탐색으로 모델 튜딩이나 앙상블 기법으로 더 좋은 성능을 찾아냄

(0509)
# SVM 이론
- 좋은 가중치w와 편향 b를 찾는 것이 필요하다
- 결정경계에 따라 도로의 넓이가 달라지기 때문에 결정경계를 잘 정하는 것이 중요(도로의 넓이가 넓을수록 일반화 성능증가)
- (조건1) 결정경계 중심으로 분류 가능
- (조건2) 결정 경계면의 기울기(절댓값 |w1|+|w2|)를 최소로 만듬(==||w||최소로 만들기, ||w||는 |w1|+|w2|제곱, (w1,w2)로 이루어진 벡터)
- 규제가 세지면 도로폭 넓어짐

---

*참고*
- 의미가 같지만 다른 문제를 쌍대문제(원문제와 의미가 같은 다른 문제를 쌍대문제라고 함)
  - LinearSVC, LinearSVR의 dual하이퍼파라미터를 사용해 true로 지정하면 쌍대문제를, false로 지정하면 원문제를 푼다. 훈련샘플의 수가 특성 수보다 큰 경우는 false로 지정한다.(ture가 기본값)

# 결정트리
- 루트(부모x 노드), 리프(부모 있는 노드)
- 노드에 포함 가능한 내용 : 분류 기준(자식노드 분류, 0~2개로 나눠질 수 있음. 부모 노드에만 포함됨),
                                              gini(노드의 지니불순도 측정값, 낮으면 좋음),
                                              samples(해당 노드에 포함 샘플 수),
                                              value(해당 노드에 포함된샘플들의 실제 클래스별 개수, 타깃 정보 활용),
                                              class(각 클래스별 비율 계싼해 가장 높은 비율의 클래스, 동일 비율이면 낮은 인덱스 선정)
- 화이트박스 모델


(5.10)
- 어떤 기준으로 분류 기준을 정하느냐 ? 무작위로 가능성을 찾아 가장 좋은 기준(지니불순도 측정 값을 참고함-> 낮을 수록 좋은 것)을 정함  (초반 들어보기)
- 불순도가 0이 아니면 노드를 더 분류할 수 있음
- n번째 노드 지니불순도 계산 : 1-(n번째 value의 첫번째 클래스 샘플 수/현재 노드의 샘플 수)^2-(~2번째~)-`````
결정경계
- max_depth에 제한을 두지 않으면 모두 분류가 가능하지만 이는 좋은 것이 아니다. 과대적합의 가능성이 높아짐.
클래스 확률 추정
- 노드의 value값으로 샘플들의 클래스별 비율을 추정함
CART훈련 알고리즘
- 손실함수(최소로 만들어야함) = 오른쪽 자식노드의 지니불순도*오른쪽 자식노드의 샘플 수 + 왼쪽, 오른쪽 자식노드의 지니불순도*왼쪽 자식노드의 샘플 수 (지니불순도 최소화하는 최적의 기준값을 찾아내야함) -> 현재의 기준값이 현재로써는 최적이나 최종적으로 최적이라는 보장은 없음(탐욕 알고리즘 사용)


entropy : 변화가 클 수록 높음

규제 파라미터
- max_depth 등
- min_~를 증가시키거나 max_~를 감소시킬 때 규제 강해짐(과대적합 막음)
