---
title: '[Blog]머신러닝 기말 정리'
layout: single
categories:
  - 시험 요약 정리
tag:
  - Blog
  - machinelearning
  - studyalone
toc: true
toc_label: "on this page"
toc_sticky: true
use_math: true
published : false
---

# 서포트벡터머신
(0502)

- 서포트 벡터 머신은 일반화를 위해 사용됨
- 마진 : 결정 경계 도로의 폭 
- 비선형 분류가 오래걸림
- 서포트 벡터 : 결정경계 도로의 가장자리에 위치한 점들
![사진](/assets/img/svm_1.jpg)

- 서포트벡터머신에서도 스케일링이 중요
- 큰 마진 분류(large margin classfication) : 마진을 최대로 하는 분류

## 🌟선형 SVM 분류
1. __하드마진분류__ : 선형분류가 가능한 경우 가능. 결정경계 도로(마진)에 아무 데이터가 없고 선형 분류로 100% 완벽하게 분류 가능
2. __소프트마진분류__ : 선형 분류로 100% 완벽하게 분류 불가능. 어느정도의 마진 오류를 허용하며 오류를 최대한 줄이며 도로 폭(마진)을 최대로 하는 분류.
  - 마진오류 : 결정경계 도로 상에 데이터가 있거나 결정경계를 넘어 데이터가 섞인 것


- __LinearSVC클래스__ : 선형회귀 지원하는 소프트벡터머신 분류기
  - __C__(규제의 강도) : 항상 양수여야하며 커질수록 규제 약해짐.클수록 결정경계의 폭이 좁아짐
- __SVC클래스__(배치경사하강법 사용)
  - 하이퍼파라미터 kernel "linear"지정해 선형분류 가능. 비선형 분류도 가능함(but 시간 오래걸림).
- __SGDClassifier 클래스__(확률적 경사하강법)
  - 선형분류, 로지스틱 회귀 가능. 하이퍼파라미터 `loss="hinge"` 지정해 서포트벡터머신 사용 가능
  - `hinge함수`는 손실함수를 나타낸다. (계산량 적음)

![사진](/assets/img/svm_2.png)

## 비선형 SVM 분류
  예시 ) `moons`데이터셋
1. __특성 추가 + 선형 SVC__
- 특성이나 데이터가 많은 경우 분류에 시간이 오래걸림, 비용 증가 등의 실현 문제점 발생 
- 다항 특성 추가 후 원하는 모델 선택
2. __🌟SVC + 커널 트릭__
- `kernel = "poly"` 하이퍼파라미터 추가로 실제로 항을 추가하지 않지만 추가한 것과 같은 효과 냄.
- `coef0` : 클수록 고차항의 중요도가 높아짐
- `C` : 규제의 정도 
3. __🌟유사도 특성__
- 임의의 샘플을 랜드마크(m)로 지정 후 특정 샘플과 각 샘플이 얼마나 유사한지를 나타내는 값이다.
- 어떤 샘플을 랜드마크로 지정할 것인가 ? ---> _모든 훈련샘플을 랜드마크로 지정해본다 = RBF_
  


(0503)  
4. __🌟가우시간 RBF 커널__
- __랜드마크에 대한 유사도를 측정한 후 특성으로 추가__ _(= 훈련데이터 수만큼 새로운 특성 추가)_
- 즉, __새로운 특성은 랜드마크와의 유사도__ _(다른 훈련셋과 얼마나 유사하냐. 얼마나 가깝냐.)_ 측정 후 
- _어떤 것을 랜드마크로 지정 > 모든 훈련셋을 랜드마크로 사용해봄. (훈련셋의 개수가 10000개면 특성 10000개가 추가됨.)_
- (참고)가우시안 방사기저함수(RBF)
  - ϕ(x,m)=exp(−γ∥x−m∥<sup>2</sup>)로 나타냄 
  - γ클수록 가까운 샘플에만 영향을 미침

> `γ`가 크면 가까운 샘플들만 유사하다고 분류  
> 규제 `C`가 크면 데이터에 민감해져 모델이 복잡해짐

    (참고)
    좋은 모델을 찾고 그리드탐색, 랜덤탐색으로 모델 튜딩이나 앙상블 기법으로 더 좋은 성능을 찾아냄
# SVM 회귀
- 결정경계 도로 위에 가능한 많은 데이터가 위치하도록 결정경계를 만듬 _(분류와의 차이)_
- ϵ 클수록 도록폭 넓어짐(도로 위에 있는 데이터가 많아짐 - 많다고 좋은 모델은 아님)

(0509)
# SVM 이론
- 결정합숫값 : h(x)= w<sup>t</sup>+b=w<sub>1</sub>x<sub>1</sub>+⋅⋅⋅+w<sub>n</sub>x<sub>n</sub>+b
> w<sub>i</sub>가 평면의 기울기를 결정함.  
> 붓꽃 데이터에선 |w<sub>1</sub>|+|w<sub>2</sub>|가 평면의 기울기를 결정하며 클수록 결정경계도로가 좁아짐 
- 결정함숫값이 음수이면 음성 (`ŷ = 0`), 0이나 양수이면 양성(`ŷ = 0`)으로 분류한다
- 좋은 `가중치(w)`와 `편향(b)`을 찾는 것이 필요하다
- __결정경계에 따라 도로의 넓이가 달라지기 때문에 결정경계를 잘 정하는 것이 중요(도로의 넓이가 넓을수록 일반화 성능증가)__
- _두 평면이 만나는 각도에 따라 결정경계가 달라짐_

> __SVM이 해결해야할 문제__ 
- (조건1) __결정경계 중심으로 분류 가능해야함__
- (조건2) __결정 경계면의 기울기(절댓값 |w<sub>1</sub>|+|w<sub>2</sub>|)를 최소로 만들어야함__(==||w||최소로 만들기, ||w||는 루트(w<sub>1</sub><sup>2</sup>+w<sub>2</sub><sup>2</sup>), (w<sub>1</sub>, w<sub>2</sub>)로 이루어진 벡터)

  나머지 내용은 강의 5-5참고하기

- 규제 `C`가 세지면 도로폭 넓어짐
- 손실함수를 완전히 만족시키면 목표로하는 조건식을 만족하기 어려워짐(모순적)
- 소프트 마진 선형SVM분류기에서 `ξ^(i)`값이 설명하는 부분은 허용오차를 말한다.
- 손실함수와 목표로하는 조건을 만족하는 w와 ξ값을 찾으면 적절한 결정경계도로를 만들 수 있음.

---

__쌍대문제__
- 의미가 같지만 다른 문제를 쌍대문제(원문제와 의미가 같은 다른 문제를 쌍대문제라고 함)
  - LinearSVC, LinearSVR의 dual하이퍼파라미터를 사용해 true로 지정하면 쌍대문제를, false로 지정하면 원문제를 푼다. 훈련샘플의 수가 특성 수보다 큰 경우는 false로 지정한다.(ture가 기본값)

# 결정트리
## 분류
- 루트(부모x 노드), 리프(부모 있는 노드)
- 지니불순도나 엔트로피를 최소화하는 기준값으로 리프노드 분류함
- 노드에 포함 가능한 내용
  - 분류 기준(자식노드 분류기준, 0~2개로 나눠질 수 있음. 부모 노드에만 포함됨)
  - gini(노드의 지니불순도 측정값, 낮으면 좋음)
  - samples(해당 노드에 포함 샘플 수)
  - value(해당 노드에 포함된샘플들의 실제 클래스별 개수, 타깃 정보 활용)
  - class(각 클래스별 비율 계싼해 가장 높은 비율의 클래스, 동일 비율이면 낮은 인덱스 선정)
- 화이트박스 모델임  
(5.10)
- 노드의 지니불순도가 0이 아니면 노드를 더 분류할 수 있음
- __n번째 노드 지니불순도__ 계산 : 1 ➖ (n번째 value의 1번째 클래스 샘플 수 / 현재 노드의 샘플 수)<sup>2</sup> ➖ (n번째 value의 2번째 클래스 샘플 수 / 현재 노드의 샘플 수)<sup>2</sup> ➖ ⋅⋅⋅⋅⋅⋅
결정경계
- max_depth에 제한을 두지 않으면 모두 분류가 가능하지만 이는 좋은 것이 아니다. 과대적합의 가능성이 높아짐.

> __클래스 확률 추정__
> - 노드의 value값으로 샘플들의 클래스별 비율을 추정함

## CART훈련 알고리즘
- 어떤 기준으로 분류 기준을 정하느냐 ? 무작위로 가능성(지니불순도)을 계산해 가장 좋은(가장 낮은) 기준을 정함
- 어떤 기준으로 분류를 할지 정하는 알고리즘(이를 통해 이진트리 만들어냄)
- 손실함수(최소로 만들어야함) 
  - = (오른쪽 자식노드의 지니불순도 × 오른쪽 자식노드의 샘플 수) ➕ (왼쪽 자식노드의 지니불순도 × 왼쪽 자식노드의 샘플 수) 
  - __지니불순도 최소화하는 최적의 기준값을 찾아내야함__ ➡️ 현재의 기준값이 현재로써는 최적이나 최종적으로 최적이라는 보장은 없음 __(탐욕 알고리즘 사용)__

> __DecisionTreeClassfier의 `criterion = 'entropy'`__  
> 지니불순도와 엔트로피 중 선택 가능함
> 엔트로피가 시간이 더 걸림
> 엔트로피가 트리의 균형을 더 잘 맞출 수 있음
> entropy : 변화가 클 수록 높음(샘플의 무질서 정도를 측정)

## 규제 파라미터
- max_depth 등
- min_~를 증가시키거나 max_~를 감소시킬 때 규제 강해짐(과대적합 막음)

(5.16)
## 회귀
- 
- x축 기준으로 쪼갬 ➡️ 해당 구간의 y의 평균값을 예측값으로 결정
- 노드에 포함 가능한 내용
  - 분류 기준(자식노드 분류기준, 0~2개로 나눠질 수 있음. 부모 노드에만 포함됨)
  - squared_error : 해당 노드에 대한 mse(데이터 포함 기준이 해당노드 부모의 분류기준으로 결정됨)
  - value : 해당 노드에 포함된 데이터의 평균값(루트노트 value는 전체 데이터 평균값)
  - samples : (해당 노드에 포함 샘플 수)
- 시간을 충분히 줄 경우 데이터에 대한 예측을 완벽하게함(좋지 않음 - 새로운 데이터에 대한 완벽한 예측은 어려움)


⭐⭐ 일반화에 대한 내용 다시 듣기(1편 40분 쯤)

## 결정트리 단점
- 훈련셋에 대한 회전민감도 __(다시 듣기)__
  - 계단식으로 예측하므로 시간이 오래걸리고 훈련셋에 회전율을 조금만 가해도 결정경계가 많이 달라짐. 
  - `PCA()`는 회전율이 가해진 데이터셋에 다시 반대로 회전율을 가해 특성 사이의 연관성을 약화시키고 결정트리 학습에 도움을 준다.
- 분산이 높음.
  - 훈련셋이나 하이퍼파라미터가 조금만 달라져도 완전히 다른 결정트리가 훈련될 수 있다.

# 앙상블 학습
- 랜덤포레스트는 앙상블학습의 예
- 일반화성능이 좋은 모델을 만든다 = 편향과 분산이 최소화되는 모델
  - 예측값과 정답이 떨어져있는 정도. 정답에 대한 잘못된 가정으로부터 유발, 편향 높으면 과소적합
  - 입력 샘플의 작은 변동에 반응하는 정도. 모델 복잡하면 편향 줄지만 분산 증가분산 크면 과대적합
  - 편향과 분산은 서로 `트레이드오프(trade-off)`되므로 적정한 모델을 찾아야함
## 투표식 분류기
- 동일한 훈련셋에 대해 여러 분류모델을 이용한 앙상블학습 진행 후 직접/간접 투표로 예측값 결정
  - 직접투표 : 다수결 방식으로(각각의 예측기보다 좋은 성능의 모델 얻을 수 있음)
  - 간접투표 : 예측 확률값들의 평균값으로 예측값 결정
    - 모든 예측기가 `predict_proba()`메서드처럼 확률 예측하는 기능 지원해야함
- 두 방식에 큰 차이가 있진 않음
- `VotinhClassfier`, `VotingRegressor`
- 성능이 좋아지는 이유는 ❓
  - 다수결을 따라서 대체로 아주 나쁜 결과가 나올 수 없음
  - 
(5.17)
