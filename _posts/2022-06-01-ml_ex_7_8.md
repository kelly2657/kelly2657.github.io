---
title: '[Blog]핸즈온머신러닝 3판 7장, 8장 연습문제'
layout: single
categories:
  - Study
tag:
  - Blog
  - machinelearning
toc: true
toc_label: "on this page"
toc_sticky: true
published: false
---
# Chap 7.

## 1️⃣
> __같은 훈련셋의 데이터로 다섯 개의 다른 모델을 훈련시켜서 모두 95%의 정확도를 얻었다면 이 모델들을 연결해 더 좋은 결과를 얻을 수 있을까?  
> 가능하다면 어떻게해야하나__
- 앙상블모델을 만들어 더 나은 결과를 기대할 수 있다. 모델이 서로 다르면 더 좋다. 베깅 기법을 사용하면 더 좋지만 모델만 달라도 좋은 결과를 낼 수 있음.

## 2️⃣
> __직접 투표와 간접 투표 분류기의 공통점과 차이점__
- 공통점 : 동일 훈련셋에 대해 서로 다른 모델로 앙상블학습 진행과정 가짐
- 차이점 : 직접투표는 앙상블학습 후 최종 예측값을 각 예측기들의 예측값으로 __다수결 투표__ 를 통해 얻는다.
           간접투표는 앙상블학습 후 최종 예측값을 각 예측기들의 예측값에 대한 확률의 평균값으로 얻으며 앙상블모델을 구성하는 예측기 모두는 `predict_proba()`와 같이 확률예측 메서드등의 기능을 가져야한다.
           
## 3️⃣
> __배깅 앙상블의 훈련을 여러대의 서버에 분산시켜 속도를 높일 수 있을까? 페이스팅, 부스팅, 랜덤포레스팅의 경우는 어떨까__
- 배깅과 페이스팅, 랜덤 포레스트 기법은 병렬 학습과 예측이 가능하다.  
하지만 부스팅 기법은 이전 예측기를 기반으로 새로운 모델을 만들어 훈련하기때문에 훈련이 순차적이어야한다. 따라서 병렬 학습과 예측에 대한 이득이 없다.

## 4️⃣
> __oob평가의 장점은 무엇인가__
- 별도의 검증셋이 없어도 편향되지 않고 앙상블모델을 평가할 수 있다.

## 5️⃣
> __엑스트라트리가 랜덤 포레스트보다 무작위성이 강조되는 이유는 무엇일까? 무작위성이 강조되었을 때의 장점과 랜덤포레스트와 엑스트라 트리의 속도 차이는 어떻게될까?__
- 랜덤포레스트는 특성만 무작위로 선택하고 특성의 임계값은 모두 계산해보지만 엑스트라트리에선 특성과 임계값을 모두 무작위로 선택해 최선의 임계값을 찾는다.  
모델의 다양성이 높아지므로 분산이 낮아지고 편향이 올라간다.  
또한 계산량이 적어 훈련속도가 빨라진다.

## 6️⃣
> __에이다부스트방식이 훈련 데이터에 과소적합되었다면 어떤 매게변수를 어떻게 바꾸어야할까?__
- 예측기 수를 증가시키거나 각 예측기의 규제 하이퍼파라미터를 감소시킨다.
  학습률을 높여 `sample_weight`가 샘플에 영향을 주는 정도를 높일 수 있다.
  
## 7️⃣
> __그레디언트 부스팅기법이 훈련데이터에 과대적합되었다면 학습률을 높여야할까?__
- 낮춰야한다. 예측기 수가 너무 많아진다면 적당한 훈련을 위해 조기종료를 사용할 수 있다.

# Chap 8.

## 1️⃣
> __차원축소의 주요 목적과 대표적 단점__
- 목적 : 알고리즘 속도 향상, 데이터 시각화, 메모리 공간 절약, 일반화성능 증가 등
- 단점 : 정보손실로 인한 알고리즘 성능 저하, 계산 비용 높음

## 2️⃣
> __차원의 저주란__
- 고차원일수록 임의의 두 지점 사이의 평균거리가 멀어지고 과대적합 위험이커짐, 훈련셋 크기를 키워야하지만 고차원에 맞는 충분한 샘플 준비는 불가능함.
         
## 3️⃣
> __데이터셋 차원축소 후 완벽히 복원할 수 있을까? 가능하다면 어떻게 가능한가. 가능하지 않다면 이유는 무엇인가.__
- 차원축소를 하며 약간이라도 데이터 손실이 발생하므로 완벽히 되돌리는 것은 불가능하지만 재구성은 가능함.

## 4️⃣
> __비선형적 데이터셋의 차원을 축소하는데 PCA 사용이 가능한가__
- 가능하다.
  만약 불필요한 차원이 없다면 PCA차원 축소는 많은 정보를 잃게한다.

## 5️⃣
> __설명분산비율을 95%로 지정한 PCA를 1000개의 차원을 가진 데이터셋에 적용한다고 가정하자.  
> 결과 데이터셋의 차원은 얼마일까?__
- 데이터셋마다 다르지만 최소 1에서 최대 950 사이의 차원을 가진다.

## 6️⃣
> __기본 PCA, 점진적 PCA, 랜덤 PCA는 어느 경우에 사용될까__
- 기본은 메모리에 맞는 경우, 점진적은 데이터의 양이 많은 경우, 랜덤은 특성의 양이 많은 경우 사용한다.
## 7️⃣
> __7. 어떤 데이터셋에 적용한 차원 축소 알고리즘의 성능을 어떻게 평가할 수 있을까?__
- 데이터 손실이 적으면 성능이 좋은 것인데 재구성했을 때 오차를 측정하면 된다. 전처리 과정으로 사용하면 성능 측정이 가능하고 원본 데이터와 비슷한 성능을 내면 좋은 것이다.

## 8️⃣
> __두개의 차원 축소 알고리즘을 연결할 수 있을까?__
- 가능하다. 예를 들어 `PCA`로 불필요 차원을 줄이고 `LLE`처럼 느린 알고리즘을 적용할 수 있다.
