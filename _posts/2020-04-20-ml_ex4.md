---
title: '[Blog]핸즈온머신러닝 2판 4장 연습문제'
layout: single
categories:
  - Study
tag:
  - Blog
  - machinelearning
toc: true
toc_label: "on this page"
toc_sticky: true
---
# Chap 4.

1. __수백만 개의 특성을 가진 훈련 세트에서는 어떤 선형 회귀 알고리즘을 사용할 수 있을까?__
> 확률적 경사하강법이나 미니배치 경사하강법을 사용할 수 있다.   
> 훈련세트가 메모리 크기에 맞으면 배치경사하강법도 사용할 수 있다.   

<확률적 경사하강법(SGD)>
- 배치크기 = 1, 스텝크기=데이터크기
- 큰 훈련세트 다를 수 있음. 외부 메모리 학습 활용 가능
- 파라미터 조정이 불안정하게 이뤄질 수 있어 지역최솟값에 덜 민감하지만 같은 이유로 전역최솟값에 수렴하지 못할 수 있음
   
<미니배치경사하강법>  
- 배치크기=2~수백(사용자 지정)
- SGD보다 파라미터 움직임이 덜 규칙적이다. 배치경사하강법보다 빠른 학습
- SGD보다 지역최솟값에 수렴할 위험도 증가
        
<배치경사하강법>   
- 배치크기 = 데이터크기, 스텝크기 = 1

배치GD-미니배치GD-SGD 순으로 최적의 파라미터 값에 수렴할 확률 높아지며 훈련시간 오래걸림


2. __훈련 세트에 있는 특성들이 각기 아주 다른 스케일을 가지고 있다. 이런 데이터에 잘 작동하지 않는 알고리즘은 무엇일까? 그 이유? 이 문제를 어떻게 해결할 수 있을까?__
> 경사하강법. 비용함수가 길쭉한 타원모양이 되기 때문이다.   
> 해결: 모델 훈련 전 스케일링을 통해 모든 테이터 특성의 척도를 통일한다.

3. __경사 하강법으로 로지스틱 회귀 모델을 훈련 시킬 때 지역 최솟값에 갇힐 가능성이 있을까?__
> 로지스틱 회귀모델의 비용함수는 볼록함수이므로 가능성이 없다

4. __충분히 오랫동안 실행하면 모든 경사 하강법 알고리즘이 같은 모델을 만들어낼까?__
> 최적화할 함수가 볼록함수이고 학습률이 너무 크지 않다고 가정하면 모든 경사하강법 알고리즘이 전역최솟값에 도달하고 비슷한 모델을 만들겠지만   
> 학습률을 점진적으로 감소시키지 않는다면 SGD와 미니배치GD는 진정한 최적점에 수렴하지 못할 것이고 전역 최적점 주변을 맴돌게 된다.   
> 즉 매우 오랫동안 훈련을 해도 경사하강법 알고리즘은 조금씩 다른 모델을 만든다

5. __배치 경사 하강법을 사용하고 에포크마다 검증 오차를 그래프로 나타내봤다. 만약 검증 오차가 일정하게 상승되고 있다면 어떤 일이 일어나고 있는 것인가? 이 문제를 어떻게 해결 할 수 있을까?__
> 학습률이 높고 알고리즘이 발산하는 것일지 모른다.   
> 훈련에러가 올라간다면 위 문제이고 학습률을 낮춰야하지만   
> 훈련에러가 올라가지 않으면 과대적합되는 것이므로 훈련을 조기종료해야한다.

6. __검증 오차가 상승하면 미니배치 경사 하강법을 즉시 중단하는 것이 좋은 방법인가__
> 지역 최솟값일수도 있으므로 더 나은 방법은 정지적으로 모델을 저장하고 최상의 검증점수를 넘어가지 못할 때 종료해 저장된 것 중 가장 좋은 모델로 복원하는 것이다.

7. __어떤 경사 하강법 알고리즘이 가장 빠르게 최적 솔루션의 주변에 도달할 수 있을까? 실제로 수렴하는 것은 어떤 것인가? 다른 방법들도 수렴하게 만들 수 있을까?__
> 배치크기가 가장 작은 SGD가 가장 먼저 솔루션 주변에 도달한다. 그 다음은 미니배치GD이다.    
> 시간이 충분하다면 배치GD만 수렴할 것이다. 학습률을 점진적으로 감소시키지 않으면 SGD와 미니배치GD는 전역최솟값 주변을 맴돌 것이다.   

8. __다항 회귀를 사용했을 때 학습 곡선을 보니 훈련 오차와 검증 오차 사이에 간격이 크다. 무슨 일이 생긴 걸까? 이 문제를 해결하는 세 가지 방법은 무엇인가?__
> 과대적합이 발생하였다.   
> 다항 차수를 낮추거나 두 오차가 줄어들 때까지 훈련데이터를 추가하거나 모델에 규제를 가한다.   

9. __릿지 회귀를 사용했을 때 훈련 오차와 검증 오차가 거의 비슷하고 둘 다 높았다. 이 모델에는 높은 편향과 높은 분산 중 어느 것이 문제인가? 규제 하이퍼파라미터  α 를 증가시켜야 할까, 아니면 줄여야 할까?__
> 과소적합이 발생했으므로 높은 편향이 문제이다.
> 
> 알파를 증가시키면 데이터에 덜 민감해지므로 감소시켜야한다.

10. __다음과 같이 사용해야 하는 이유는?__
- 평범한 선형 회귀(즉, 아무런 규제가 없는 모델) 대신 릿지 회귀
- 릿지 회귀 대신 라쏘 회귀
- 라쏘 회귀 대신 엘라스틱넷
> 릿지회귀는 일반적으로 추천된다. 규제가 있는 모델이 없는 모델보다 성능이 좋기 때문
> 
> 유용하지 않은 특성이 많이 포함되어있는 데이터로 훈련할 때 라쏘회귀가 추천된다.   
>
> 특성 수가 훈련샘플 수보다 크거나 특정 특성이 강하게 연관되어있을 경우   

11. __사진을 낮과 밤, 실내와 실외로 분류하려고 한다. 두 개의 로지스틱 회귀 분류기를 만들어야 할까, 하나의 소프트맥스 회귀 분류기를 만들어야 할까?__
> 네가지 조합이 모두 가능하므로 두개의 로지스틱 회귀 분류기가 좋다.
